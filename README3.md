# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ3
### –ó–∞–¥–∞–Ω–∏–µ ‚Ññ1
```Python
import re
import unicodedata


def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    # –ó–∞–º–µ–Ω—è–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    text = ''.join(char if char.isprintable() or char.isspace() else ' ' for char in text)
    text = re.sub(r'\s+', ' ', text)
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    return text.strip()
print('normalize')
print("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t ->", normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print("\"—ë–∂–∏–∫, –Å–ª–∫–∞\" ->", normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
print("\"Hello\r\nWorld\" ->", normalize("Hello\r\nWorld"))
print("\"  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  \" ->", normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))


def tokenize(text: str) -> list[str]:
    pattern = r'[\w]+(?:-[\w]+)*'
    tokens = re.findall(pattern, text)
    return tokens
print('tokenize')
print("\"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä\" ->", tokenize(("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")))
print("\"hello,world!!!\" ->", tokenize(("hello,world!!!")))
print("\"–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ\" ->", tokenize(("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ")))
print("\"2025 –≥–æ–¥\" ->", tokenize(("2025 –≥–æ–¥")))
print("\"emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ\" ->", tokenize(("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ")))

def count_freq(tokens: list[str]) -> dict[str, int]:
    freq = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1
    return freq

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, –∑–∞—Ç–µ–º –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —Å–ª–æ–≤–∞ (–∞–ª—Ñ–∞–≤–∏—Ç—É)
    sorted_items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))
    return sorted_items[:n]
print('count_freq + top_n')
print('["a","b","a","c","b","a"] ->', count_freq((["a","b","a","c","b","a"])))
print('top_n(..., n=2) ->', top_n(count_freq((["a","b","a","c","b","a"]))))
print('["bb","aa","bb","aa","cc"] ->', count_freq((["bb","aa","bb","aa","cc"])))
print('top_n(..., n=2) ->', top_n(count_freq((["bb","aa","bb","aa","cc"]))))
```
![](images/lab03/ex01.l3.jpg)

### –ó–∞–¥–∞–Ω–∏–µ ‚Ññ2
``` Python
from text import normalize, tokenize, count_freq, top_n

def text_stats(text: str):
    norm = normalize(text)
    tokens = tokenize(norm)
    freq = count_freq(tokens)
    
    print(f'–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}')
    print(f'–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(freq)}')
    print('–¢–æ–ø-5:')
    
    top = top_n(freq, 5)  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ top_n –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤—Ç–æ—Ä–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    for key, value in top:
        print(f"{key}: {value}")
print('–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç!!!')
text_stats('–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç!!!')
```
![](images/lab03/ex02.l3.jpg)

